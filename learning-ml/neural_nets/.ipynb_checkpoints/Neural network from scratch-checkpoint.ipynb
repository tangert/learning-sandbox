{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to take an object oriented approach and make this as intuitive as possible.\n",
    "\n",
    "Testing this out on MNIST (the handwritten digit dataset), then will convert it into a Tensorflow NN. Current plan is to use Tensorflow.js and create a web app that shows the training process in real-time with D3 + React. Might also do a visualization in 3dsmax/cinema4d or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length: 70000\n",
      "Training data length: 63000\n",
      "Testing data length: 7000\n",
      "Targets:  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the MNIST dataset\n",
    "Contains 70,000 examples of hand-written digits in 28x28 pixel form (784 item array)\n",
    "So the shape is (70000, 784)\n",
    "\"\"\"\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# The original data\n",
    "# Stored as a 70,000 x 784 array\n",
    "data = mnist[\"data\"]\n",
    "\n",
    "# Normalize the data because every grayscale value is out of 255\n",
    "data = data/255\n",
    "\n",
    "# Take first 90% for training, last 10% for testing\n",
    "training_proportion = 0.9\n",
    "training_index = math.floor(len(data)*training_proportion)\n",
    "\n",
    "training_data = data[0 : training_index]\n",
    "testing_data = data[training_index : len(data)]\n",
    "\n",
    "print(\"Total data length:\",len(data))\n",
    "print(\"Training data length:\",len(training_data))\n",
    "print(\"Testing data length:\",len(testing_data))\n",
    "\n",
    "# The target labels for classifying the data\n",
    "# These are used to actually train the network for a given input\n",
    "targets = mnist[\"target\"]\n",
    "target_vals = list(set(targets))\n",
    "\n",
    "# use the training targets to calculate output error during back propogation\n",
    "training_targets = targets[0 : training_index]\n",
    "\n",
    "# use the testing targets to predict accuracy\n",
    "testing_targets = targets[training_index : len(targets)]\n",
    "\n",
    "print(\"Targets: \", target_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A basic 3 layer feed forward neural network\n",
    "    \n",
    "    The goal here is to abstract each component of the network (neuron, layer, synapses)\n",
    "    to a high level to easily visualize how the network operates and how it trains\n",
    "    \n",
    "    The network records the state of the weights/neurons at each epoch and exports\n",
    "    a JSON file representing the entire training process\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the network given input layer,\n",
    "        hidden layer, and output layer neuron size\n",
    "        \"\"\"\n",
    "        \n",
    "        # DEFINE LAYERS\n",
    "        # Each layer is an array of Neuron objects\n",
    "        self.input_layer = Layer(size=input_size, name=\"Input\")\n",
    "        self.hidden_layer = Layer(size=hidden_size, name=\"Hidden\")\n",
    "        self.output_layer = Layer(size=output_size, name=\"Output\")\n",
    "        \n",
    "        # Define a larger object which contains all layers\n",
    "        # for easy iteration\n",
    "        self.layers = [self.input_layer,\n",
    "                       self.hidden_layer,\n",
    "                       self.output_layer]\n",
    "        \n",
    "        # DEFINE SYNAPSES\n",
    "        # Represented by matrices\n",
    "        # input -> hidden layer synapses\n",
    "        self.input_to_hidden_synapses = Synapses(input_size, hidden_size)\n",
    "        \n",
    "        # hidden -> output synapses\n",
    "        self.hidden_to_output_synapses = Synapses(hidden_size, output_size)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the network\n",
    "        Prints the layers with the synapses in between\n",
    "        \"\"\"\n",
    "        delim = \"\\n\"\n",
    "        return delim.join([str(layer) for layer in self.layers])\n",
    "        \n",
    "    #########################################\n",
    "    # MARK: Training and processing functions\n",
    "    #########################################\n",
    "    \n",
    "    def train(self, data, targets, iterations, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the network\n",
    "        \n",
    "        Data:\n",
    "        The training data represented as 784-dimensional vectors\n",
    "        \n",
    "        Targets:\n",
    "        The sample labels\n",
    "        \n",
    "        Iterations: how many times we train the network\n",
    "            - on each iteration of training, predict an output and update\n",
    "            the weights of the network via back propogation\n",
    "        \n",
    "        Learning rate: the size of the \"steps\" that gradient descent takes\n",
    "            - higher learning rate means larger steps but also risks\n",
    "              \"overstepping\" the minimum of the curve\n",
    "            - low learning rate is more precise but calculating the gradient itself\n",
    "              is computationally expensive so there is a good middle ground\n",
    "        \"\"\"\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # grab the expected result from the targets/labels array\n",
    "            expected = targets[i]\n",
    "            \n",
    "            # grab the relevant training sample to test against the expected label\n",
    "            training_sample = np.asmatrix(data[i])\n",
    "            \n",
    "            # calculate the predicted value via forward propogation\n",
    "            prediction = self.forward_propogate(training_sample)\n",
    "            \n",
    "            print(\"\\nExpected: \", expected)\n",
    "            print(\"Prediction: \", prediction)\n",
    "            \n",
    "            # update the weights of the network via backward propogation\n",
    "            self.back_propogate(training_sample, expected, prediction)\n",
    "                \n",
    "    def forward_propogate(self, data, using_matrix_multiplication=True):\n",
    "        \"\"\"\n",
    "        Passes an input data vector (an image grid, a list of tuples, etc) through\n",
    "        the network and outputs a prediction from the output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # matrix multiplication allows us to compute weight x input calculations\n",
    "        # in large batches and is super efficient\n",
    "        if using_matrix_multiplication:\n",
    "            \n",
    "            # First, multiply the data and synapse matrices to sum up each\n",
    "            # input / weight combination\n",
    "            self.input_to_hidden_sum = np.dot(data,\n",
    "                                              self.input_to_hidden_synapses.weights)\n",
    "                        \n",
    "            # Pass the entire summed matrix into the sigmoid function\n",
    "            self.input_to_hidden_activated = self.activate(self.input_to_hidden_sum)\n",
    "            \n",
    "            # Multiply the activated input/hidden layer by\n",
    "            # the second set of weights\n",
    "\n",
    "            self.hidden_to_output_sum = np.dot(self.input_to_hidden_activated,\n",
    "                                               self.hidden_to_output_synapses.weights)\n",
    "            \n",
    "            # Finally, pass the last sum of hidden -> output neurons\n",
    "            # into the sigmoid\n",
    "            final_prediction = self.activate(self.hidden_to_output_sum)\n",
    "                    \n",
    "        # otherwise, manually compute the sums of weights/inputs in each neuron\n",
    "        \n",
    "        # first pass the data from the input neurons to the hidden layer\n",
    "        # multiply all of the inputs by their respective weights\n",
    "        # add them up at each neuron\n",
    "        # pass that through the activation function\n",
    "            # you can call the function on an entire matrix\n",
    "        \n",
    "        #second pass that new number and multiply by second set of weights\n",
    "        # add all of them up in the ouput neuron\n",
    "        # apply the activation one more time\n",
    "        # that's your prediction!\n",
    "        \n",
    "        return final_prediction\n",
    "    \n",
    "    def back_propogate(self, data, expected, output):\n",
    "        \"\"\"\n",
    "        Updates the weights of the network based on training data to make the\n",
    "        network more accurate\n",
    "        \"\"\"\n",
    "        self.output_err = expected - output\n",
    "        self.output_delta = self.output_err*self.activate(output, deriv=True)\n",
    "        \n",
    "        self.hidden_layer_err = self.output_delta.dot(self.hidden_to_output_synapses.weights.T)\n",
    "        self.hidden_layer_delta = self.hidden_layer_err*self.activate(self.input_to_hidden_activated, deriv=True)\n",
    "        \n",
    "        self.input_to_hidden_synapses.update(data.T.dot(self.hidden_layer_delta))\n",
    "        self.hidden_to_output_synapses.update(self.input_to_hidden_activated.T.dot(self.output_delta))\n",
    "    \n",
    "    def activate(self, x, deriv=False):\n",
    "        \"\"\"\n",
    "        activation sigmoid function\n",
    "        takes in the summed weights * inputs\n",
    "        \"\"\"\n",
    "        if deriv:\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    ##########################################\n",
    "    # MARK: Export and visualization functions\n",
    "    ##########################################\n",
    "    \n",
    "    def export_network(self):\n",
    "        \"\"\"\n",
    "        Exports the network as a list of objects that\n",
    "        represent the entire state of the network\n",
    "        \n",
    "        Can use this for visualization purposes\n",
    "        \n",
    "        1. Network object: \n",
    "        {\n",
    "            input_neurons: [],\n",
    "            training_states: [\n",
    "                { State 1 }, \n",
    "                { State 2 },\n",
    "                { State 3 },\n",
    "                ...\n",
    "                { State n (n = number of epochs) }\n",
    "            ],\n",
    "            output_neurons: []\n",
    "        }\n",
    "        \n",
    "            State object:\n",
    "            (note: only needs to include the variables that change)\n",
    "            {\n",
    "                 input_to_hidden_synapses: [], -> matrix of weights\n",
    "                 hidden_to_output_synapses: [], -> matrix of weights\n",
    "                 hidden_layer: [] -> list of tuples: (sum, activated_sum)\n",
    "            }\n",
    "        \"\"\"\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Stores a list of neurons\n",
    "    Initialized with the number of neurons in the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, size, name):\n",
    "        self.name = name\n",
    "        self.neurons = [Neuron(neuron_id=i) for i in range(size)]\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Prints out the name of the layer and the neurons\n",
    "        \"\"\"\n",
    "        s = \"Layer: {}\\n\".format(self.name)\n",
    "        delim = \"\\n\"           \n",
    "        return s + delim.join([str(n) for n in self.neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Takes in data and applies an activation function to pass to the next layer\n",
    "    \n",
    "    These functions are really only used if we don't use matrix multiplication to\n",
    "    do all of the operations at once...\n",
    "    \n",
    "    But it's important to understand how this is happening on an individual-neuron level\n",
    "    \"\"\"\n",
    "    \n",
    "    # 'states' stores all of the previous values for visualization\n",
    "    # Format: [(sum, activated_sum)] list of tuples containing the sum, then activated sum\n",
    "    # This way we can visualize the process of the data actually being passed into the neuron\n",
    "    # and then being processed\n",
    "        \n",
    "    def __init__(self, neuron_id):\n",
    "        # Stores a single integer as the neuron's ID to identify which weights are relevant to it\n",
    "        # during forward propogation\n",
    "        self.neuron_id = neuron_id\n",
    "        \n",
    "        # Stores all of the previous values for visualization\n",
    "        # Format: [(sum, activated_sum)] list of tuples containing the sum, then activated sum\n",
    "        # This way we can visualize the process of the data actually being passed into the neuron\n",
    "        # and then being processed\n",
    "        self.states = []\n",
    "    \n",
    "    def process_input(self, weights, inputs):\n",
    "        \"\"\"\n",
    "        performs matrix multiplication between the weights and the inputs\n",
    "        then passes it to the activation function\n",
    "        \n",
    "        for example:\n",
    "        for neuron 1:\n",
    "        obtain all weights of form W11, W21, W31...WN1\n",
    "        obtain all input neuron values I1, I2, I3...IN\n",
    "        multiply each of them together and them add them up\n",
    "        \n",
    "        pass that sum to the activation function\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def activate(self, x, deriv=False):\n",
    "        \"\"\"\n",
    "        activation sigmoid function\n",
    "        takes in the summed weights * inputs\n",
    "        \"\"\"\n",
    "        if deriv:\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def save_current_state(self, processed, activated):\n",
    "        \"\"\"\n",
    "        Saves the current state of the neuron into the\n",
    "        states array given the processed and activated values\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def __str__(show_data=False):\n",
    "        \"\"\"\n",
    "        Prints out the number of the neuron\n",
    "        Shows the current sum and activated sum if specified\n",
    "        \"\"\"\n",
    "        space = \"    \"\n",
    "        return space + \"Neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synapses:\n",
    "    \"\"\" \n",
    "    The connections in between layers\n",
    "    Represented as a matrix of weights\n",
    "    \n",
    "    The synapses store all of its previous weights to visualize the training process\n",
    "    \"\"\"\n",
    "    def __init__(self, rows, colums):\n",
    "        # this initializes the weights to a random Column-d array\n",
    "        self.weights = np.random.randn(rows, colums)\n",
    "    \n",
    "    def __str__():\n",
    "        \"\"\"\n",
    "        Prints out the weights and the labels for each weight\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "        \n",
    "    # updates the weights\n",
    "    def update(new_weights):\n",
    "        self.weights += new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example network \n",
    "nn = NeuralNetwork(input_size=784,\n",
    "                   hidden_size=15,\n",
    "                   output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected:  0.0\n",
      "Prediction:  [[0.26416604]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,15) and (1,15) not aligned: 15 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-115dd2cc35e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m          learning_rate=0.01)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-292-0c86885ebada>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, targets, iterations, learning_rate)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# update the weights of the network via backward propogation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_propogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musing_matrix_multiplication\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-292-0c86885ebada>\u001b[0m in \u001b[0;36mback_propogate\u001b[0;34m(self, data, expected, output)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_to_output_synapses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer_err\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_hidden_activated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_hidden_synapses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-292-0c86885ebada>\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(self, x, deriv)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,15) and (1,15) not aligned: 15 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "nn.train(data=training_data,\n",
    "         targets=training_targets,\n",
    "         iterations=1000,\n",
    "         learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
