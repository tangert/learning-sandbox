{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to take an object oriented approach and make this as intuitive as possible.\n",
    "\n",
    "Testing this out on MNIST (the handwritten digit dataset), then will convert it into a Tensorflow NN. Current plan is to use Tensorflow.js and create a web app that shows the training process in real-time with D3 + React. Might also do a visualization in 3dsmax/cinema4d or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length: 70000\n",
      "Training data length: 63000\n",
      "Testing data length: 7000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the MNIST dataset\n",
    "Contains 70,000 examples of hand-written digits in 28x28 pixel form (784 item array)\n",
    "\"\"\"\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# The original data\n",
    "# Stored as a 70,000 x 784 array\n",
    "data = mnist[\"data\"]\n",
    "\n",
    "# Take first 90% for training, last 10% for testing\n",
    "training_proportion = 0.9\n",
    "training_index = math.floor(len(data)*training_proportion)\n",
    "\n",
    "training_data = data[0 : training_index]\n",
    "testing_data = data[training_index : len(data)]\n",
    "\n",
    "print(\"Total data length:\",len(data))\n",
    "print(\"Training data length:\",len(training_data))\n",
    "print(\"Testing data length:\",len(testing_data))\n",
    "\n",
    "# The target labels for classifying the data\n",
    "# [0,1,2,3,4,5,6,7,8,9]\n",
    "targets = mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A basic 3 layer feed forward neural network\n",
    "    \n",
    "    The goal here is to abstract each component of the network (neuron, layer, synapses)\n",
    "    to a high level to easily visualize how the network operates and how it trains\n",
    "    \n",
    "    The network records the state of the weights/neurons at each epoch and exports\n",
    "    a JSON file representing the entire training process\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the network given input layer,\n",
    "        hidden layer, and output layer neuron size\n",
    "        \"\"\"\n",
    "        \n",
    "        # DEFINE LAYERS\n",
    "        # Each layer is an array of Neuron objects\n",
    "        self.input_layer = Layer(size=input_size, name=\"Input\")\n",
    "        self.hidden_layer = Layer(size=hidden_size, name=\"Hidden\")\n",
    "        self.output_layer = Layer(size=output_size, name=\"Output\")\n",
    "        \n",
    "        # Define a larger object which contains all layers\n",
    "        # for easy iteration\n",
    "        self.layers = [self.input_layer,\n",
    "                       self.hidden_layer,\n",
    "                       self.output_layer]\n",
    "        \n",
    "        # DEFINE SYNAPSES\n",
    "        # Represented by matrices\n",
    "        # input -> hidden layer synapses\n",
    "        self.input_to_hidden_synapses = Synapses(input_size, hidden_size)\n",
    "        \n",
    "        # hidden -> output synapses\n",
    "        self.hidden_to_output_synapses = Synapses(hidden_size, output_size)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the network\n",
    "        Prints the layers with the synapses in between\n",
    "        \"\"\"\n",
    "        delim = \"\\n\"\n",
    "        return delim.join([str(layer) for layer in self.layers])\n",
    "        \n",
    "    #########################################\n",
    "    # MARK: Training and processing functions\n",
    "    #########################################\n",
    "    \n",
    "    def train(self, data, expected, epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the network\n",
    "        \n",
    "        Epochs: the number of iterations\n",
    "            - on each iteration of training, predict an output and update\n",
    "            the weights of the network via back propogation\n",
    "        \n",
    "        Learning rate: the size of the \"steps\" that gradient descent takes\n",
    "            - higher learning rate means larger steps but also risks\n",
    "              \"overstepping\" the minimum of the curve\n",
    "            - low learning rate is more precise but calculating the gradient itself\n",
    "              is computationally expensive so there is a good middle ground\n",
    "        \"\"\"\n",
    "        \n",
    "        for gen in range(epochs):\n",
    "            output = self.forward_propogate(data)\n",
    "            self.backward_propogate(data, expected, output)            \n",
    "                \n",
    "    def forward_propogate(self, data, using_matrix_multiplication=True):\n",
    "        \"\"\"\n",
    "        Passes an input data vector (an image grid, a list of tuples, etc) through\n",
    "        the network and outputs a prediction from the output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # matrix multiplication allows us to compute weight x input calculations\n",
    "        # in large batches and is super efficient\n",
    "        if using_matrix_multiplication:\n",
    "            \n",
    "            # First, multiply the data and synapse matrices to sum up each\n",
    "            # input / weight combination\n",
    "            self.input_to_hidden_sum = np.dot(data,\n",
    "                                              self.input_to_hidden_synapses.weights)\n",
    "                        \n",
    "            # Pass the entire summed matrix into the sigmoid function\n",
    "            self.input_to_hidden_activated = self.activate(self.input_to_hidden_sum)\n",
    "            \n",
    "            # Multiply the activated input/hidden layer by\n",
    "            # the second set of weights\n",
    "\n",
    "            self.hidden_to_output_sum = np.dot(self.input_to_hidden_activated,\n",
    "                                               self.hidden_to_output_synapses.weights)\n",
    "            \n",
    "            # Finally, pass the last sum of hidden -> output neurons\n",
    "            # into the sigmoid\n",
    "            final_prediction = self.activate(self.hidden_to_output_sum)\n",
    "                    \n",
    "        # otherwise, manually compute the sums of weights/inputs in each neuron\n",
    "        \n",
    "        # first pass the data from the input neurons to the hidden layer\n",
    "        # multiply all of the inputs by their respective weights\n",
    "        # add them up at each neuron\n",
    "        # pass that through the activation function\n",
    "            # you can call the function on an entire matrix\n",
    "        \n",
    "        #second pass that new number and multiply by second set of weights\n",
    "        # add all of them up in the ouput neuron\n",
    "        # apply the activation one more time\n",
    "        # that's your prediction!\n",
    "        \n",
    "        return final_prediction\n",
    "    \n",
    "    def backward_propogate(self, data, expected, output):\n",
    "        \"\"\"\n",
    "        Updates the weights of the network based on training data to make the\n",
    "        network more accurate\n",
    "        \"\"\"\n",
    "        self.output_error = expected - output\n",
    "        self.output_delta = self.output_error*self.activate(output, deriv=True)\n",
    "        \n",
    "        self.hidden_layer_error = self.output_delta.dot(self.hidden_to_output_synapses.weights.T)\n",
    "        self.hidden_layer_delta = self.hidden_layer_error*self.activate(self.input_to_hidden_activated, deriv=True)\n",
    "        \n",
    "        self.input_to_hidden_synapses.update(data.T.dot(self.hidden_layer_delta))\n",
    "        self.hidden_to_output_synapses.update(self.input_to_hidden_activated.T.dot(self.output_delta))\n",
    "    \n",
    "    def activate(self, x, deriv=False):\n",
    "        \"\"\"\n",
    "        activation sigmoid function\n",
    "        takes in the summed weights * inputs\n",
    "        \"\"\"\n",
    "        if deriv:\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    ##########################################\n",
    "    # MARK: Export and visualization functions\n",
    "    ##########################################\n",
    "    \n",
    "    def export_network(self):\n",
    "        \"\"\"\n",
    "        Exports the network as a list of objects that\n",
    "        represent the entire state of the network\n",
    "        \n",
    "        Can use this for visualization purposes\n",
    "        \n",
    "        1. Network object: \n",
    "        {\n",
    "            input_neurons: [],\n",
    "            training_states: [\n",
    "                { State 1 }, \n",
    "                { State 2 },\n",
    "                { State 3 },\n",
    "                ...\n",
    "                { State n (n = number of epochs) }\n",
    "            ],\n",
    "            output_neurons: []\n",
    "        }\n",
    "        \n",
    "            State object:\n",
    "            (note: only needs to include the variables that change)\n",
    "            {\n",
    "                 input_to_hidden_synapses: [], -> matrix of weights\n",
    "                 hidden_to_output_synapses: [], -> matrix of weights\n",
    "                 hidden_layer: [] -> list of tuples: (sum, activated_sum)\n",
    "            }\n",
    "        \"\"\"\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Stores a list of neurons\n",
    "    Initialized with the number of neurons in the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, size, name):\n",
    "        self.name = name\n",
    "        self.neurons = [Neuron() for _ in range(size)]\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Prints out the name of the layer and the neurons\n",
    "        \"\"\"\n",
    "        s = \"Layer: {}\\n\".format(self.name)\n",
    "        delim = \"\\n\"           \n",
    "        return s + delim.join([str(n) for n in self.neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Takes in data and applies an activation function to pass to the next layer\n",
    "    \n",
    "    These functions are really only used if we don't use matrix multiplication to\n",
    "    do all of the operations at once...\n",
    "    \n",
    "    But it's important to understand how this is happening on an individual-neuron level\n",
    "    \"\"\"\n",
    "    \n",
    "    # 'states' stores all of the previous values for visualization\n",
    "    # Format: [(sum, activated_sum)] list of tuples containing the sum, then activated sum\n",
    "    # This way we can visualize the process of the data actually being passed into the neuron\n",
    "    # and then being processed\n",
    "    \n",
    "    states = []\n",
    "    \n",
    "    def process_input(self, weights, inputs):\n",
    "        \"\"\"\n",
    "        performs matrix multiplication between the weights and the inputs\n",
    "        then passes it to the activation function\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def activate(self, x, deriv=False):\n",
    "        \"\"\"\n",
    "        activation sigmoid function\n",
    "        takes in the summed weights * inputs\n",
    "        \"\"\"\n",
    "        if deriv:\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def save_current_state(self, processed, activated):\n",
    "        \"\"\"\n",
    "        Saves the current state of the neuron into the\n",
    "        states array given the processed and activated values\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def __str__(show_data=False):\n",
    "        \"\"\"\n",
    "        Prints out the number of the neuron\n",
    "        Shows the current sum and activated sum if specified\n",
    "        \"\"\"\n",
    "        space = \"    \"\n",
    "        return space + \"Neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synapses:\n",
    "    \"\"\" \n",
    "    The connections in between layers\n",
    "    Represented as a matrix of weights\n",
    "    \n",
    "    The synapses store all of its previous weights to visualize the training process\n",
    "    \"\"\"\n",
    "    def __init__(self, rows, colums):\n",
    "        # this initializes the weights to a random Column-d array\n",
    "        self.weights = np.random.randn(rows, colums)\n",
    "    \n",
    "    def __str__():\n",
    "        \"\"\"\n",
    "        Prints out the weights and the labels for each weight\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "        \n",
    "    # updates the weights\n",
    "    def update(new_weights):\n",
    "        self.weights += new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example network \n",
    "nn = NeuralNetwork(input_size=2,\n",
    "                   hidden_size=3,\n",
    "                   output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Input\n",
      "    Neuron\n",
      "    Neuron\n",
      "Layer: Hidden\n",
      "    Neuron\n",
      "    Neuron\n",
      "    Neuron\n",
      "Layer: Output\n",
      "    Neuron\n"
     ]
    }
   ],
   "source": [
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46024277])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward_propogate([2,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
