{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object oriented approach.\n",
    "\n",
    "Technically a basic multilayer perceptron.\n",
    "\n",
    "Testing this out on MNIST (the handwritten digit dataset), then will convert it into a Tensorflow NN. Current plan is to use Tensorflow.js and create a web app that shows the training process in real-time with D3 + React. Might also do a visualization in 3dsmax/cinema4d or something.\n",
    "\n",
    "The data that this network exports can come in two forms\n",
    "\n",
    "1. Entire training history: on each training iteration, all of the weights, the activation values and processed inputs for each neuron, the prediction, cost, etc are all recorded. This is essentially a snapshot of the entire network.\n",
    "\n",
    "2. Results on each iteration: only the cost, prediction, and expected value are recorded. This is much lighter weight and can be exported as a csv.\n",
    "\n",
    "TO IMPLEMENT:\n",
    "1. Stochastic gradient descent. Currently the network is just randomly shuffling all of the data and going through each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length: 70000\n",
      "Training data length: 60000\n",
      "Testing data length: 10000\n",
      "Targets:  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the MNIST dataset\n",
    "Contains 70,000 examples of hand-written digits in 28x28 pixel form (784 item array)\n",
    "So the shape is (70000, 784)\n",
    "\"\"\"\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# The original data\n",
    "# Stored as a 70,000 x 784 array\n",
    "data = mnist[\"data\"]\n",
    "\n",
    "# Normalize the data because every grayscale value is out of 255\n",
    "data = data/255\n",
    "\n",
    "# Take first 60,000 samples for training, last 10,000 for testing\n",
    "sample_size = 60000\n",
    "training_data = data[:sample_size]\n",
    "testing_data = data[sample_size:]\n",
    "\n",
    "print(\"Total data length:\",len(data))\n",
    "print(\"Training data length:\",len(training_data))\n",
    "print(\"Testing data length:\",len(testing_data))\n",
    "\n",
    "# The target labels for classifying the data\n",
    "# These are used to actually train the network for a given input\n",
    "targets = mnist[\"target\"]\n",
    "target_vals = list(set(targets))\n",
    "\n",
    "# use the training targets to calculate output error during back propogation\n",
    "training_targets = targets[0 : sample_size]\n",
    "\n",
    "# use the testing targets to predict accuracy\n",
    "testing_targets = targets[sample_size : len(targets)]\n",
    "\n",
    "# shuffle the training data and targets so it doesn't overfit\n",
    "training_data, training_targets = shuffle(training_data, training_targets, random_state=0)\n",
    "\n",
    "print(\"Targets: \", target_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A basic 3 layer feed forward neural network\n",
    "    \n",
    "    The goal here is to abstract each component of the network (neuron, layer, synapses)\n",
    "    to a high level to easily visualize how the network operates and how it trains\n",
    "    \n",
    "    The network records the state of the weights/neurons at each epoch and exports\n",
    "    a JSON file representing the entire training process\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the network given input layer,\n",
    "        hidden layer, and output layer neuron size\n",
    "        \"\"\"\n",
    "        \n",
    "        # DEFINE LAYERS\n",
    "        # Each layer is an array of Neuron objects\n",
    "        self.input_layer = Layer(size=input_size, name=\"Input\")\n",
    "        self.hidden_layer = Layer(size=hidden_size, name=\"Hidden\")\n",
    "        self.output_layer = Layer(size=output_size, name=\"Output\")\n",
    "        \n",
    "        # DEFINE SYNAPSES\n",
    "        # Represented by matrices\n",
    "        # input -> hidden layer synapses\n",
    "        self.input_to_hidden_synapses = Synapses(input_size, hidden_size)\n",
    "        \n",
    "        # hidden -> output synapses\n",
    "        self.hidden_to_output_synapses = Synapses(hidden_size, output_size)\n",
    "        \n",
    "        # States stores \"snapshots\" of the network at each training iteration\n",
    "        self.training_states = []\n",
    "\n",
    "        # Results stores the results of eachiteration of the network (a slightly minified state)\n",
    "        self.results = []\n",
    "        \n",
    "        #store the initial state of the network\n",
    "        self.initial_state = {\n",
    "            \"w0\": self.input_to_hidden_synapses.weights.tolist(),\n",
    "            \"w1\": self.hidden_to_output_synapses.weights.tolist()\n",
    "        }\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the network\n",
    "        Prints the layers with the synapses in between\n",
    "        \"\"\"\n",
    "        delim = \"\\n\"\n",
    "        return delim.join([str(layer) for layer in self.layers])\n",
    "        \n",
    "    #########################################\n",
    "    # MARK: Training and processing functions\n",
    "    #########################################\n",
    "    \n",
    "    def test(self, data, targets):\n",
    "        \"\"\"\n",
    "        Tests the network on an input data set\n",
    "        \"\"\"\n",
    "        # keeps the count of correct guesses\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(len(targets)):\n",
    "            \n",
    "            test_sample = data[i]\n",
    "            prediction = self.forward_propagate(test_sample)\n",
    "            \n",
    "            expected_digit = targets[i]\n",
    "            predicted_digit = np.argmax(prediction)\n",
    "                        \n",
    "            if expected_digit == predicted_digit:\n",
    "                correct+=1\n",
    "        \n",
    "        accuracy = correct / len(targets)\n",
    "        return accuracy\n",
    "            \n",
    "        \n",
    "    def train(self, data, targets, iterations, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the network\n",
    "        \n",
    "        Data:\n",
    "        The training data represented as 784-dimensional vectors\n",
    "        \n",
    "        Targets:\n",
    "        The sample labels\n",
    "        \"\"\"\n",
    "                \n",
    "        for i in range(iterations):\n",
    "                        \n",
    "            # grab the expected result from the targets/labels array\n",
    "            self.expected = self.vectorize_digit(int(targets[i]))\n",
    "            \n",
    "            # grab the relevant training sample to test against the expected label\n",
    "            self.training_sample = data[i]\n",
    "            \n",
    "            # calculate the predicted value via forward propogation\n",
    "            self.prediction = self.forward_propagate(self.training_sample)\n",
    "\n",
    "            # cost vector\n",
    "            costv = self.expected - self.prediction\n",
    "            \n",
    "            # cost is the sum of the squares of the differences\n",
    "            self.cost = sum([(num)**2 for num in costv])\n",
    "                \n",
    "            # update the weights of the network via backward propogation\n",
    "            self.back_propagate(self.training_sample, self.expected, self.prediction, learning_rate)\n",
    "            \n",
    "            # Save the state every 1000 iterations\n",
    "            # print out the cost of the network every 1000 iterations\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Iteration {} | cost: {}\".format(i, self.cost))\n",
    "                self.save_state()\n",
    "\n",
    "            # on every iteration, save the predictions, costs, and expected\n",
    "            self.save_result()\n",
    "            \n",
    "    def forward_propagate(self, data):\n",
    "        \"\"\"\n",
    "        Passes an input data vector (an image grid, a list of tuples, etc) through\n",
    "        the network and outputs a prediction from the output layer\n",
    "        \n",
    "        rewrite eventually to handle amount of layers\n",
    "        \"\"\"\n",
    "                    \n",
    "        # First, multiply the data and synapse matrices to sum up each\n",
    "        # input / weight combination\n",
    "        self.input_to_hidden_sum = np.dot(data,\n",
    "                                          self.input_to_hidden_synapses.weights)\n",
    "\n",
    "        # Pass the entire summed matrix into the sigmoid function\n",
    "        self.input_to_hidden_activated = self.activate(self.input_to_hidden_sum)\n",
    "\n",
    "        # Multiply the activated input/hidden layer by\n",
    "        # the second set of weights\n",
    "\n",
    "        self.hidden_to_output_sum = np.dot(self.input_to_hidden_activated,\n",
    "                                           self.hidden_to_output_synapses.weights)\n",
    "        \n",
    "        # Set the states of each neuron after processing and activation\n",
    "        for i, n in enumerate(self.hidden_layer.neurons):\n",
    "            processed = self.input_to_hidden_sum[i]\n",
    "            activated = self.input_to_hidden_activated[i]\n",
    "            n.set_state(processed,activated)\n",
    "\n",
    "        # Finally, pass the last sum of hidden -> output neurons\n",
    "        # into the sigmoid\n",
    "        output = self.activate(self.hidden_to_output_sum)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    # FIXME: Implement learning rate\n",
    "    def back_propagate(self, data, expected, output, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the weights of the network based on training data to make the\n",
    "        network more accurate        \n",
    "        \"\"\"\n",
    "        data = data.reshape(len(data), 1)\n",
    "            \n",
    "        output_err = expected - output\n",
    "        output_delta = output_err*self.activate(output, deriv=True)\n",
    "        \n",
    "        # Calculate the hidden layer errors and deltas\n",
    "        hidden_layer_err = np.dot(output_delta, self.hidden_to_output_synapses.weights.T)\n",
    "        hidden_layer_delta = hidden_layer_err*self.activate(self.input_to_hidden_activated, deriv=True)\n",
    "        \n",
    "        # MARK: Reshape\n",
    "        # Need to convert into 1-dimensional vectors so the matrix multiplication works\n",
    "        hidden_layer_delta = hidden_layer_delta.reshape(1, len(hidden_layer_delta))\n",
    "        output_delta = output_delta.reshape(1, len(output_delta))\n",
    "        \n",
    "        hidden_to_input = self.input_to_hidden_activated.T\n",
    "        hidden_to_input = hidden_to_input.reshape(len(hidden_to_input), 1)\n",
    "        \n",
    "        # Final new calculations\n",
    "        new_input_to_hidden = np.dot(data, hidden_layer_delta)\n",
    "        new_hidden_to_output = np.dot(hidden_to_input, output_delta)\n",
    "\n",
    "        # Update the weights\n",
    "        self.input_to_hidden_synapses.update(new_input_to_hidden)\n",
    "        self.hidden_to_output_synapses.update(new_hidden_to_output)\n",
    "    \n",
    "    \n",
    "    def activate(self, x, deriv=False):\n",
    "        \"\"\"\n",
    "        activation sigmoid function\n",
    "        takes in the summed weights * inputs\n",
    "        \"\"\"\n",
    "        if deriv:\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def vectorize_digit(self, digit):\n",
    "        \"\"\" \n",
    "        converts a digit into the 10 dimensional column vector used by the NN\n",
    "        \"\"\"\n",
    "        v = np.zeros(10)\n",
    "        v[digit] = 1\n",
    "        return v\n",
    "    \n",
    "    ##########################################\n",
    "    # MARK: Export and visualization functions\n",
    "    ##########################################\n",
    "    \n",
    "    def save_state(self):\n",
    "        \"\"\"\n",
    "        Saves the current state of the network\n",
    "        Essentially a snapshot of the network throughout a single training example\n",
    "        \"\"\"\n",
    "        current_state = {\n",
    "            \"w0\": self.input_to_hidden_synapses.weights,\n",
    "            \"w1\": self.hidden_to_output_synapses.weights,\n",
    "            \"hidden\": [n.state for n in self.hidden_layer.neurons],\n",
    "            \"data\": self.training_sample,\n",
    "            \"prediction\": self.prediction,\n",
    "            \"expected\": self.expected,\n",
    "            \"cost\": self.cost\n",
    "        }\n",
    "        self.training_states.append(current_state)\n",
    "\n",
    "    def save_result(self):\n",
    "        current_result = {\n",
    "            \"prediction\": self.prediction,\n",
    "            \"expected\":self.expected,\n",
    "            \"cost\": self.cost\n",
    "        }\n",
    "        self.results.append(current_result)\n",
    "    \n",
    "\n",
    "    def export_predictions(self, file):\n",
    "        \"\"\"\n",
    "        Exports the predictions, expectations, and the cost of the network\n",
    "        as a CSV \n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(\"{}.csv\".format(file))\n",
    "\n",
    "\n",
    "    def export_network(self, file):\n",
    "        \"\"\"\n",
    "        Exports the network as a list of objects that\n",
    "        represent the entire state of the network\n",
    "        \n",
    "        Can use this for visualization purposes\n",
    "        \n",
    "        1. Network object:\n",
    "        {\n",
    "           { Initial state }, -> Object that represents the initial weights of the nework\n",
    "            [ -> List of training states, costs, predictions\n",
    "                { State 1 }, \n",
    "                { State 2 },\n",
    "                { State 3 },\n",
    "                ...\n",
    "                { State n (n = number of epochs) }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "            Internal state object:\n",
    "            {\n",
    "                 input_to_hidden_synapses: [], -> matrix of weights\n",
    "                 hidden_to_output_synapses: [], -> matrix of weights\n",
    "                 hidden_layer: [] -> list of tuples: (sum, activated_sum)\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store the entire network as an object\n",
    "        n = {\n",
    "            \"initial_state\": self.initial_state,\n",
    "            \"training_states\": self.training_states\n",
    "        }\n",
    "                \n",
    "        print(\"Exporting network\")\n",
    "\n",
    "        # Exports a pretty printed version of the network\n",
    "        with open(\"{}-indented.json\".format(file), 'w') as f:\n",
    "            output = json.dump(n, f, cls=NumpyEncoder, indent=4, ensure_ascii=False, separators=(',', ': '))\n",
    "\n",
    "        # Exports a minified version of the network (about half the size)\n",
    "        with open(\"{}-min.json\".format(file), 'w') as f:\n",
    "            output = json.dump(n, f, cls=NumpyEncoder, ensure_ascii=False)\n",
    "            \n",
    "        print(\"Done exporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Stores a list of neurons\n",
    "    Initialized with the number of neurons in the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, size, name):\n",
    "        self.name = name\n",
    "        self.neurons = [Neuron(neuron_id=i) for i in range(size)]\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Prints out the name of the layer and the neurons\n",
    "        \"\"\"\n",
    "        s = \"Layer: {}\\n\".format(self.name)\n",
    "        delim = \"\\n\"           \n",
    "        return s + delim.join([str(n) for n in self.neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Stores all of the previous values for visualization\n",
    "    This way we can visualize the process of the data actually being passed into the neuron\n",
    "    and then being processed\n",
    "    \"\"\"\n",
    "    \n",
    "    # 'states' stores all of the previous values for visualization\n",
    "    # Format: [(sum, activated_sum)] list of tuples containing the sum, then activated sum\n",
    "    # This way we can visualize the process of the data actually being passed into the neuron\n",
    "    # and then being processed\n",
    "        \n",
    "    def __init__(self, neuron_id):\n",
    "        # Stores a single integer as the neuron's ID to identify which weights are relevant to it\n",
    "        # during forward propogation\n",
    "        self.neuron_id = neuron_id\n",
    "        \n",
    "        # Stores all of the previous values for visualization\n",
    "        # Format: [(sum, activated_sum)] list of tuples containing the sum, then activated sum\n",
    "        # This way we can visualize the process of the data actually being passed into the neuron\n",
    "        # and then being processed\n",
    "        self.state = { \"processed\": 0, \"activated\" : 0 }\n",
    "    \n",
    "    def set_state(self, processed, activated):\n",
    "        \"\"\"\n",
    "        Saves the current state of the neuron into the\n",
    "        states array given the processed and activated values\n",
    "        \"\"\"\n",
    "        self.state[\"processed\"] = processed\n",
    "        self.state[\"activated\"] = activated     \n",
    "    \n",
    "    def __str__(show_data=False):\n",
    "        \"\"\"\n",
    "        Prints out the number of the neuron\n",
    "        Shows the current sum and activated sum if specified\n",
    "        \"\"\"\n",
    "        space = \"    \"\n",
    "        return space + self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synapses:\n",
    "    \"\"\" \n",
    "    The connections in between layers\n",
    "    Represented as a matrix of weights\n",
    "    \n",
    "    The synapses store all of its previous weights to visualize the training process\n",
    "    \"\"\"\n",
    "    def __init__(self, rows, colums):\n",
    "        # this initializes the weights to a random Column-d array\n",
    "        self.weights = np.random.randn(rows, colums)\n",
    "        \n",
    "    # updates the weights\n",
    "    def update(self, new_weights):\n",
    "        self.weights += new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "            np.int16, np.int32, np.int64, np.uint8,\n",
    "            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
    "            np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example network \n",
    "# input = 784 dimensional image vector\n",
    "# 15 hidden neurons\n",
    "# output = 10 neurons for each possible digit\n",
    "\n",
    "nn = NeuralNetwork(input_size=784,\n",
    "                   hidden_size=15,\n",
    "                   output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | cost: 4.4666539435599235\n",
      "Iteration 1000 | cost: 1.0298489734064151\n",
      "Iteration 2000 | cost: 0.15361936086584804\n",
      "Iteration 3000 | cost: 0.4933474858380852\n",
      "Iteration 4000 | cost: 0.03721624842163593\n",
      "Iteration 5000 | cost: 0.320524226943579\n",
      "Iteration 6000 | cost: 0.13110371556643413\n",
      "Iteration 7000 | cost: 0.026976019033628\n",
      "Iteration 8000 | cost: 0.0056304523954631555\n",
      "Iteration 9000 | cost: 0.752487029584936\n",
      "Iteration 10000 | cost: 0.0017410397266728393\n",
      "Iteration 11000 | cost: 1.0383672372902366\n",
      "Iteration 12000 | cost: 0.006072066707377591\n",
      "Iteration 13000 | cost: 0.4736000157292362\n",
      "Iteration 14000 | cost: 0.009530840103221874\n",
      "Iteration 15000 | cost: 0.025570190964187983\n",
      "Iteration 16000 | cost: 0.0011512122574391405\n",
      "Iteration 17000 | cost: 0.0005504488141769038\n",
      "Iteration 18000 | cost: 0.00048519265853783913\n",
      "Iteration 19000 | cost: 0.02254373945039315\n",
      "Iteration 20000 | cost: 0.0026891240344524593\n",
      "Iteration 21000 | cost: 0.0007933383551697551\n",
      "Iteration 22000 | cost: 0.0004476227857507342\n",
      "Iteration 23000 | cost: 1.7227401881568767\n",
      "Iteration 24000 | cost: 0.0005473934341581297\n",
      "Iteration 25000 | cost: 0.006781654344278205\n",
      "Iteration 26000 | cost: 0.0001679333402631217\n",
      "Iteration 27000 | cost: 0.005317017744524979\n",
      "Iteration 28000 | cost: 0.0031856122492252793\n",
      "Iteration 29000 | cost: 0.029616846848853195\n",
      "Iteration 30000 | cost: 0.0010608266806154695\n",
      "Iteration 31000 | cost: 0.0005511344808011598\n",
      "Iteration 32000 | cost: 0.5444562696325839\n",
      "Iteration 33000 | cost: 0.0037462289925940424\n",
      "Iteration 34000 | cost: 0.0026603134033896937\n",
      "Iteration 35000 | cost: 1.1346263414723683\n",
      "Iteration 36000 | cost: 0.01757054628621155\n",
      "Iteration 37000 | cost: 0.2930499970711431\n",
      "Iteration 38000 | cost: 0.14395806888217447\n",
      "Iteration 39000 | cost: 3.172692330557305e-05\n",
      "Iteration 40000 | cost: 0.7910946701756105\n",
      "Iteration 41000 | cost: 0.00035906572616088467\n",
      "Iteration 42000 | cost: 0.0008482567861722463\n",
      "Iteration 43000 | cost: 1.0084201169113212\n",
      "Iteration 44000 | cost: 0.006177887193211426\n",
      "Iteration 45000 | cost: 0.2001817065233347\n",
      "Iteration 46000 | cost: 0.09880640157106196\n",
      "Iteration 47000 | cost: 0.00017340247331859243\n",
      "Iteration 48000 | cost: 0.0007976565251653087\n",
      "Iteration 49000 | cost: 6.06915626141976e-05\n",
      "Iteration 50000 | cost: 0.0017211708714557612\n",
      "Iteration 51000 | cost: 0.0002403712889922141\n",
      "Iteration 52000 | cost: 0.00039283334528190044\n",
      "Iteration 53000 | cost: 0.0007076483684672305\n",
      "Iteration 54000 | cost: 0.004846188401981228\n",
      "Iteration 55000 | cost: 0.0002468741609022864\n",
      "Iteration 56000 | cost: 0.03265060792815565\n",
      "Iteration 57000 | cost: 0.000732797828089633\n",
      "Iteration 58000 | cost: 0.005055862133155749\n",
      "Iteration 59000 | cost: 0.0025115211035170344\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "nn.train(data=training_data,\n",
    "         targets=training_targets,\n",
    "         iterations=60000,\n",
    "         learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network accuracy:  0.8928\n"
     ]
    }
   ],
   "source": [
    "# Test the network\n",
    "accuracy = nn.test(data=testing_data,\n",
    "                   targets=testing_targets)\n",
    "\n",
    "print(\"Network accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.export_predictions(\"trained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
